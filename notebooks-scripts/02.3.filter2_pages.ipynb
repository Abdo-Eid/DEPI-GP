{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fe3c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import wikipediaapi\n",
    "import time\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a43124b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. إعداد المسارات بدقة ---\n",
    "INPUT_FILE = 'output/agriculture_pages_optimized.json'  # <- updated to pages JSON\n",
    "OUTPUT_FILE = 'output/Final_agriculture_pages_ar.jsonl'  # <- updated output file\n",
    "\n",
    "# --- 2. إعداد الاتصال بويكيبيديا ---\n",
    "wiki_wiki = wikipediaapi.Wikipedia(\n",
    "    user_agent='MyAgriResearch/2.0 (research@example.com)',\n",
    "    language='ar',\n",
    "    extract_format=wikipediaapi.ExtractFormat.WIKI,\n",
    "    timeout=15  # زيادة الوقت لتجنب الانقطاع\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8e02b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2308 unique page titles.\n"
     ]
    }
   ],
   "source": [
    "# 1. قراءة ملف الصفحات\n",
    "with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "    unique_titles = json.load(f)\n",
    "print(f\"Loaded {len(unique_titles)} unique page titles.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f58eee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from urllib.parse import unquote\n",
    "\n",
    "def get_article_details(title):\n",
    "    \"\"\"جلب تفاصيل المقالة من ويكيبيديا\"\"\"\n",
    "    try:\n",
    "        page = wiki_wiki.page(title)\n",
    "        \n",
    "        if not page.exists():\n",
    "            return None\n",
    "        \n",
    "        return {\n",
    "            \"article_id\": str(page.pageid),\n",
    "            \"title\": page.title,\n",
    "            \"url\": unquote(page.fullurl),\n",
    "            \"summary\": page.summary.replace('\\n', ' ').strip(),\n",
    "            \"full_text\": page.text.replace('\\n', ' ').strip()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# example\n",
    "article = get_article_details(random.choice(unique_titles))\n",
    "article"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f76e428",
   "metadata": {},
   "source": [
    "# Cleaning the full text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee3686e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(f\"<div style='white-space: pre-wrap; word-wrap: break-word;'>{article['full_text']}</div>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2ef9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_wiki_text(text):\n",
    "    \"\"\"تنظيف النصوص من علامات الويكي والمراجع وحذف الأقسام المحددة بالكامل\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # الأقسام المراد حذفها مع نصها بالكامل\n",
    "    end_patterns = [\n",
    "        r'==\\s*المراجع\\s*==',\n",
    "        r'==\\s*وصلات خارجية\\s*==',\n",
    "        r'==\\s*انظر أيضًا\\s*==',\n",
    "        r'==\\s*الهوامش\\s*=='\n",
    "    ]\n",
    "    \n",
    "    # البحث عن أول ظهور لأي قسم وحذف كل شيء بعده\n",
    "    first_cut = len(text)\n",
    "    for pattern in end_patterns:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            first_cut = min(first_cut, match.start())\n",
    "    \n",
    "    text = text[:first_cut]\n",
    "    \n",
    "    # إزالة المراجع الرقمية [1], [2], ...\n",
    "    text = re.sub(r'\\[\\d+\\]', '', text)\n",
    "    \n",
    "    # إزالة أي وسوم wiki مثل [عدل]\n",
    "    text = re.sub(r'\\[.*?]', '', text)\n",
    "    \n",
    "    # إزالة المسافات الزائدة\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def is_valid_article(page_obj, cleaned_text):\n",
    "    # استبعاد صفحات التوضيح\n",
    "    if \"توضيح\" in page_obj.categories:\n",
    "        return False, 0\n",
    "    \n",
    "    # استبعاد المقالات القصيرة جدًا\n",
    "    word_count = len(cleaned_text.split())\n",
    "    if word_count < 150:\n",
    "        return False, word_count\n",
    "        \n",
    "    return True, word_count\n",
    "\n",
    "display(HTML(f\"<div style='white-space: pre-wrap; word-wrap: break-word;'>{clean_wiki_text(article['full_text'])}</div>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9ce280",
   "metadata": {},
   "source": [
    "# getting the full dataset in jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfa6654",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing articles: 100%|██████████| 2308/2308 [39:13<00:00,  1.02s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved 2308 articles to output/Final_agriculture_pages_ar.jsonl (JSONL format)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import unquote\n",
    "\n",
    "\n",
    "# Process all articles, clean text, validate, and save to output JSONL file\n",
    "with open(OUTPUT_FILE, 'w', encoding='utf-8') as f_out:\n",
    "    for title in tqdm(unique_titles, desc=\"Processing articles\"):\n",
    "        try:\n",
    "            # Get article details\n",
    "            page = wiki_wiki.page(title)\n",
    "            if not page.exists():\n",
    "                continue\n",
    "\n",
    "            # Clean the full text\n",
    "            cleaned_text = clean_wiki_text(page.text.replace('\\n', ' ').strip())\n",
    "\n",
    "            # Validate the article\n",
    "            is_valid = is_valid_article(page, cleaned_text)\n",
    "\n",
    "            if is_valid[0]:  # assuming is_valid returns (True, word_count)\n",
    "                # Prepare article data\n",
    "                article_data = {\n",
    "                    \"article_id\": str(page.pageid),\n",
    "                    \"title\": page.title,\n",
    "                    \"url\": unquote(page.fullurl),\n",
    "                    \"summary\": page.summary.replace('\\n', ' ').strip(),\n",
    "                    \"full_text\": cleaned_text,\n",
    "                    \"word_count\": is_valid[1]\n",
    "                }\n",
    "                # Write as one JSON line\n",
    "                f_out.write(json.dumps(article_data, ensure_ascii=False) + \"\\n\")\n",
    "                f_out.flush()  # optional, ensures data is saved immediately\n",
    "\n",
    "            time.sleep(0.1)  # polite delay to avoid hitting Wikipedia too fast\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing article: {title} error {e}\")\n",
    "            continue\n",
    "\n",
    "print(f\"Processed and saved {len(unique_titles)} articles to {OUTPUT_FILE} (JSONL format)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cc8e2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "second_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
