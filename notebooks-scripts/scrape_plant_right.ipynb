{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ff500f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install requests beautifulsoup4 markdownify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e54db84",
   "metadata": {},
   "source": [
    "This script scrapes agricultural guides for five specific crops from AgriCash, automatically visiting their detailed pages to convert HTML content into clean Markdown. It then compiles the plant names, source URLs, and formatted text into a structured JSONL dataset optimized for training Q&A models.##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f78492cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. جاري الدخول للصفحة الرئيسية...\n",
      "-- تم العثور على محصول: الذرة الشامية والرفعية\n",
      "   جاري سحب التفاصيل من: https://agricash.app/corn-and-sorghum/\n",
      "-- تم العثور على محصول: الشمر\n",
      "   جاري سحب التفاصيل من: https://agricash.app/%D8%AE%D8%AF%D9%85%D8%A9%20%D9%88%20%D8%B2%D8%B1%D8%A7%D8%B9%D8%A9%20%D8%A7%D9%84%D8%B4%D9%85%D8%B1/\n",
      "Error fetching https://agricash.app/%D8%AE%D8%AF%D9%85%D8%A9%20%D9%88%20%D8%B2%D8%B1%D8%A7%D8%B9%D8%A9%20%D8%A7%D9%84%D8%B4%D9%85%D8%B1/: 404 Client Error: Not Found for url: https://agricash.app/%D8%AE%D8%AF%D9%85%D8%A9%20%D9%88%20%D8%B2%D8%B1%D8%A7%D8%B9%D8%A9%20%D8%A7%D9%84%D8%B4%D9%85%D8%B1/\n",
      "-- تم العثور على محصول: الريحان\n",
      "   جاري سحب التفاصيل من: https://agricash.app/%d8%ae%d8%af%d9%85%d8%a9-%d9%88%d8%b2%d8%b1%d8%b9%d8%a9-%d8%a7%d9%84%d8%b1%d9%8a%d8%ad%d8%a7%d9%86/\n",
      "-- تم العثور على محصول: البصل\n",
      "   جاري سحب التفاصيل من: https://agricash.app/onion-cultivation/\n",
      "-- تم العثور على محصول: بنجر السكر\n",
      "   جاري سحب التفاصيل من: https://agricash.app/sugar-beet/?\n",
      "3. جاري الحفظ في plant_right.jsonl...\n",
      "تمت المهمة بنجاح!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from markdownify import markdownify as md\n",
    "import json\n",
    "import time\n",
    "\n",
    "# إعدادات المتصفح لتجنب الحظر\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "def get_soup(url):\n",
    "    \"\"\"دالة مساعدة لجلب محتوى الصفحة وتحويله لـ Soup\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS)\n",
    "        response.raise_for_status()\n",
    "        return BeautifulSoup(response.content, 'html.parser')\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def scrape_plant_right():\n",
    "    base_url = \"https://agricash.app/plant-right/\"\n",
    "    output_file = \"plant_right.jsonl\"\n",
    "    \n",
    "    print(\"1. جاري الدخول للصفحة الرئيسية...\")\n",
    "    soup = get_soup(base_url)\n",
    "    if not soup:\n",
    "        return\n",
    "\n",
    "    \n",
    "    target_crops = [\"الذرة\", \"الشمر\", \"الريحان\", \"البصل\", \"بنجر\"]\n",
    "    extracted_data = []\n",
    "\n",
    "    all_headings = soup.find_all(['h2', 'h3', 'h4', 'h5'])\n",
    "    \n",
    "    for heading in all_headings:\n",
    "        text = heading.get_text(strip=True)\n",
    "        \n",
    "        # التأكد أن العنوان هو أحد المحاصيل المستهدفة\n",
    "        if any(crop in text for crop in target_crops):\n",
    "            print(f\"-- تم العثور على محصول: {text}\")\n",
    "            \n",
    "            plant_name = text\n",
    "            \n",
    "        \n",
    "            card = heading.find_parent() \n",
    "            \n",
    "            summary = \"\"\n",
    "            summary_tag = heading.find_next_sibling('p')\n",
    "            if summary_tag:\n",
    "                summary = summary_tag.get_text(strip=True)\n",
    "            \n",
    "            # البحث عن رابط التفاصيل \"إضغط هنا\" أو الرابط المباشر\n",
    "            link_tag = heading.find_next('a', href=True)\n",
    "            detailed_url = link_tag['href'] if link_tag else None\n",
    "            \n",
    "            if detailed_url:\n",
    "                # معالجة الرابط إذا كان نسبيًا (Relative URL)\n",
    "                if not detailed_url.startswith('http'):\n",
    "                    detailed_url = f\"https://agricash.app{detailed_url}\"\n",
    "                \n",
    "                print(f\"   جاري سحب التفاصيل من: {detailed_url}\")\n",
    "                \n",
    "                # --- الخطوة ب: سحب الصفحة التفصيلية ---\n",
    "                detailed_markdown = get_detailed_content(detailed_url)\n",
    "                \n",
    "                # --- الخطوة ج: تنسيق المخرج ---\n",
    "                # التنسيق المطلوب: # [Crop Name][Summary]---[Content]\n",
    "                full_markdown = f\"# {plant_name}\\n{summary}\\n---\\n{detailed_markdown}\"\n",
    "                \n",
    "                record = {\n",
    "                    \"plant_name\": plant_name,\n",
    "                    \"source_url\": detailed_url,\n",
    "                    \"markdown_content\": full_markdown\n",
    "                }\n",
    "                \n",
    "                extracted_data.append(record)\n",
    "                \n",
    "                # انتظار بسيط لتجنب الضغط على السيرفر\n",
    "                time.sleep(1)\n",
    "\n",
    "    # حفظ البيانات في ملف JSONL\n",
    "    print(f\"3. جاري الحفظ في {output_file}...\")\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for entry in extracted_data:\n",
    "            json.dump(entry, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "    \n",
    "    print(\"تمت المهمة بنجاح!\")\n",
    "\n",
    "def get_detailed_content(url):\n",
    "    \"\"\"دالة لسحب محتوى الصفحة التفصيلية وتنظيفه وتحويله لمارك داون\"\"\"\n",
    "    soup = get_soup(url)\n",
    "    if not soup:\n",
    "        return \"\"\n",
    "    \n",
    "    # محاولة تحديد المحتوى الأساسي فقط (تجاهل الهيدر والفوتر)\n",
    "    # عادة ما يكون المحتوى داخل <article> أو <main> أو div class=\"content\"\n",
    "    content_div = soup.find('article') or soup.find('main') or soup.find('div', class_='entry-content')\n",
    "    \n",
    "    if not content_div:\n",
    "        # إذا فشل التحديد، خذ البودي كله (خيار أخير)\n",
    "        content_div = soup.body\n",
    "\n",
    "    # تنظيف العناصر غير المرغوب فيها قبل التحويل\n",
    "    for unwanted in content_div.find_all(['script', 'style', 'nav', 'footer', 'header', 'form']):\n",
    "        unwanted.decompose()\n",
    "\n",
    "    # تحويل HTML إلى Markdown\n",
    "    # heading_style='ATX' يجعل العناوين تبدأ بـ # بدلاً من خط تحتها\n",
    "    markdown_text = md(str(content_div), heading_style=\"ATX\")\n",
    "    \n",
    "    # تنظيف إضافي للنص الناتج\n",
    "    lines = []\n",
    "    for line in markdown_text.split('\\n'):\n",
    "        line = line.strip()\n",
    "        # حذف جملة \"إضغط هنا\" أو الروابط الفارغة\n",
    "        if \"إضغط هنا\" in line or line == \"\":\n",
    "            continue\n",
    "        lines.append(line)\n",
    "        \n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_plant_right()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1012536e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed images from 5 items and saved to 'plant_right_cleaned_no_images.jsonl'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# Load JSONL\n",
    "with open('plant_right.jsonl', 'r', encoding='utf-8') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "def remove_images(md_content):\n",
    "    \"\"\"\n",
    "    Remove all Markdown image tags from the content\n",
    "    \"\"\"\n",
    "    # Regex to match Markdown images ![alt](url)\n",
    "    md_no_images = re.sub(r'!\\[.*?\\]\\(.*?\\)', '', md_content)\n",
    "    return md_no_images\n",
    "\n",
    "# Apply cleaning\n",
    "for item in data:\n",
    "    item['markdown_content'] = remove_images(item['markdown_content'])\n",
    "\n",
    "# Save cleaned JSONL\n",
    "with open('plant_right.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for item in data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"Removed images from {len(data)} items and saved to 'plant_right_cleaned_no_images.jsonl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58926313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import os\n",
    "\n",
    "# # Load JSONL\n",
    "# with open('plant_right_cleaned_no_images.jsonl', 'r', encoding='utf-8') as f:\n",
    "#     data = [json.loads(line) for line in f]\n",
    "\n",
    "\n",
    "# with open('plant_right_first_item.md', 'w', encoding='utf-8') as md_file:\n",
    "#     # \\n in string will create real line breaks in the .md file\n",
    "#     md_file.write(data[0]['markdown_content'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
